For the 'iris' dataset, I employed 3 clustering models: 1 for all predictors, 1 for sepal predictors and 1 for petal predictors. Then I picked the optimal on by comparing the distance sum of all points to their cluster centers in each model. Optimal model should have the lower distance sum for a given cluster number.

I am also interested in seeing how my models performed compared to actual responses so I pulled them into tables to compare.

```{r setup, results='hide'}
library(ggplot2) #for ploting the cluster
set.seed(93)
```

Calling the iris dataset from R's built-in datasets:

``` {r iris}
data(iris)
head(iris)
```

There are 5 columns with 4 predictors and 1 responses ('Species'). Sepal lengths, sepal widths, petal lengths, and petal widths correspond to column 1, 2, 3, and 4 respectively.

I see that there are differences in magnitude of the values in the 4 predictors, for example, Petal Width values are less than 1. I think it is a good idea to scale the data before the analysis. The scaling here is linear, which gives back data from 0 to 1.

``` {r scale}
scaled_iris <- iris

for (i in 1:4){
  
  scaled_iris[,i] <- (iris[,i] - min(iris[,i])) / (max(iris[,i]) - min(iris[,i]))
}

head(scaled_iris)
```

Of the reponses, I am interested to see how many actual species there are in the dataset:

``` {r species}

summary(iris['Species'])
```

There are total 3 unique species in the dataset with 50 data points in for each.

Now I will build a function that will return a clustering model for a given range of predictor columns and a given value of cluster center:

``` {r model}
cluster_model = function(data, start_col = 1, end_col = 4, center = 2, nstart = 25){
  set.seed(93)
  cluster_model = kmeans(data[,start_col:end_col], center, nstart = nstart)
  return (cluster_model)
}
```

Finally, before I dive into the iris data, I will build a function that will run k-means clustering model on for a given range of predictor columns and a given range of cluster center a return a table that will compare cluster centers and the corresponding sum of distance to the corresponding center.

``` {r compare_cluster}

compare_cluster = function(data, start_col = 1, end_col = 4, min_center = 2, max_center = 5, nstart = 25){
  
  comparison_list <- list()
  
  for (c in min_center:max_center){
    set.seed(93)
    model = cluster_model(data,start_col = start_col, end_col = end_col, center = c, nstart = nstart) #calling the previous function 

    dist_sum = 0 
    
    for (i in 1:nrow(data)){
      
      dist_sum = dist_sum + sqrt(sum((data[i,start_col:end_col] - model$centers[model$cluster[i],])^2)) #calculate Euclidean distance sum between points and their cluster centers
    }
    
    comparison_list[[c]] = data.table(centroid=c, distance_sum=dist_sum)
  }
  
  comparison_table = rbindlist(comparison_list)
  
  return (comparison_table)
}
```

First let's consider the case of using all 4 predictors (column 1 to 4) for clustering:

I will look at a range of possible clusters from 2 to 10 and see which clusters is optimal based on the sum of distance between one datapoints and its cluster:

```{r allpred}

all_pred_table = compare_cluster(scaled_iris,start_col = 1, end_col = 4, min_center = 2, max_center = 10)

all_pred_table
```

To acutally pick the optimal number of centroids, let's plot an Elbow Diagram for the table:

``` {r elbow}

ggplot(all_pred_table,aes(centroid, distance_sum)) + geom_line(color='blue')+ geom_point()
```

The most signficance change in distance sum is from 2 to 3, after that the degree of change is less severe. Hence, the clustering model for all predictors is more optimal with at least 3 cluster centers.

Next, I want to see if using sepal lenghts and widths as predictors only would yield any difference:

``` {r sepal}

sepal_pred_table = compare_cluster(scaled_iris,start_col = 1, end_col = 2, min_center = 2, max_center = 10)

sepal_pred_table

ggplot(sepal_pred_table,aes(centroid, distance_sum)) + geom_line(color='blue')+ geom_point()
```

Similarly to using all predictors, distance sum changes signficantly from from 2 to 3, but after that the degree of change of change is smoother. 

However, I notice that at 3, the distance sum for using Sepal as predictors only is 21.05, while with all predictors it's 29.22. Since the goal is to minimize the distance sum for a given number of cluster, model using sepal predictors only seems to perform better than one with all predictors.

Lastly, I will run the same thing again, this time using petal lengths and petal widths. Technically since there are 4 predictors, if I want a clustering model that runs on 2 predictors I could have 6 possible combinations. I just do not think that it makes since to run a model on, for example, sepal lengths and petal widths.


``` {r petal}

petal_pred_table = compare_cluster(scaled_iris,start_col = 3, end_col = 4, min_center = 2, max_center = 10)

petal_pred_table

ggplot(petal_pred_table,aes(centroid, distance_sum)) + geom_line(color='blue')+ geom_point()
```

Same as the above, this model would be optimal with at least 3 cluster centers. 

But the important thing is that at 3 clusters, this model produces the least distance sum at 13.34 among the 3 models that I ran. Therefore, I conclude that the clustering model using only petal widths and petal lengths as predictors is more optimal.

Now that I have determined the optimal model, let's run and plot the scatter plot of this model for 3 cluster centers to compare with the actual responses. This is for the sake of the analysis that I picked 3 cluster centers to be the same with the number of actual species in this dataset. In real life, actual responses should not be known since clustering is unsupervised learning.

``` {r plot}

petal_model = cluster_model(scaled_iris, start_col = 3, end_col = 4, center = 3)
petal_model
```

Let's now, for the sake of the exercise, plot a table to compared between predicted cluster of our 3 models above and actual responses:

``` {r comps}
all_model = cluster_model(scaled_iris, start_col = 1, end_col = 4, center = 3)
sepal_model = cluster_model(scaled_iris, start_col = 1, end_col = 2, center = 3)

table(all_model$cluster, scaled_iris$Species) #combining predicted clusters and actual species names together

table(sepal_model$cluster, scaled_iris$Species)

table(petal_model$cluster, scaled_iris$Species)
```

All-predictor model misclassified 14 virginca, 3 versicolor while sepal-predictor model misclassified 16 virginaca and 13 versicolor.

The 3 clusters size of petal model are 48, 52, 50. Compared to the actual species size in the data of 50, 50, 50. It seems that only 2 'versicolor' datapoints and 4 'virginca' are misclassified with 'setosa' perfecly classified. Hence, this reassures that the clustering model with petal lengths and petal widths as the predictors is optimal.

``` {r scatter}
petal_model$cluster <- as.factor(petal_model$cluster) #change cluster resutls from continuous to discrete scale

ggplot(scaled_iris, aes(Petal.Length, Petal.Width, color = petal_model$cluster)) + geom_point()
```


The scatter plot seems very clean and each data points colored by their predicted cluster.



