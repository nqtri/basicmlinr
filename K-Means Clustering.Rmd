---
title: "Week 2 Homework Submission"
date: "January 19, 2019"
output: html_document
---

## Question 3.1

In this question, I employed cross validation for both KNN model and SVM model. For KNN model, I tested with 10 fold cross validation to find the optimal k. Then I looped through a variety of folds and k values to find the optimal combinations. Once that is done, I plugged k into the test set to test the actual accuracy of the model. Similarly, I repeated the process for SVM model, only replacing a range of k values with a range of c values.

For part b, I also ran training, validation, and testing on both KNN and SVM models with a split of 60/20/20 and then concluded with a good classifier for both.

Setting up by calling relevant libraries and opening data file and assigning to 'mydata' variable:

```{r set up, results='hide'}
library(kernlab)
library(kknn)
library(data.table)
set.seed(93) #to achieve reproductible results
```

```{r openfile}
mydata <- read.table('credit_card_data-headers.txt',header = TRUE) #open data file with  headers then set header parameter to TRUE

head(mydata) #First 6 rows of the data
tail(mydata) #Last 6 rows of the data
```

### Part a

First, I still need to split the data into 2 sets: training and test sets where cross validation will be performed on training set. The ratio of training/testing I'd use here is 80/20. Here, I'll use random sampling selection process.

``` {r datasplit}
set.seed(93)
test_portion = sample(1:nrow(mydata),0.2*nrow(mydata), replace = FALSE) #replace = FALSE means whatever data is taken out will not be put back for selection again

cross_test_set = mydata[test_portion,] #data set used for testing

cross_train_set = mydata[-test_portion,] #data set used for training

head(cross_test_set)

head(cross_train_set)
```

#### KNN Model

Next, I create a function to find the accuracy range for a given range of k (nearest neighbors) and a given value of k fold by which cross validation is perform:

``` {r cal}
accuracy_cal = function(data, max_k = 20, k_fold = 10){ #max_k = max number of k, here, i consider a k range of 1 to 20
  
  accuracy_range = rep(0,max_k) # set up a range of 50 zeros as a place holder so that prediction accuracy numbers can be inserted in later
  
  for (k in 1:max_k) {
    set.seed(93) #setting seed to achieve reproductible results, any number is ok
    cv_model = cv.kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15,
                       data, 
                       kcv = k_fold, #indicate that cross validation to be performed
                       k = k,
                       scale = TRUE)
    
    prediction = as.integer(round(cv_model[[1]][,2],0))
    
    accuracy_range[[k]] = sum(prediction == data[,11]) / nrow(data)
  }
  return(accuracy_range)
}
```

Then, let's see the accuracy range for a default value of max_k = 20 and k_fold = 10 on the training set:

``` {r acc_range}
accuracy_cal(cross_train_set)
```

Let's see how that plots and locate the highest prediction accuracy and the corresponding k nearest neighbors:

```{r acc_plot}
plot(accuracy_cal(cross_train_set))
```

```{r max}
max(accuracy_cal(cross_train_set))
which.max(accuracy_cal(cross_train_set))
```

As it is shown here, for a 10-fold cross validation knn model, the best prediction accuracy for credit card approval is around 84.54% and the corresponding k value for the optimal number of nearest neighbours in this case is 5.

Now, I will write a function that can loop through a range of k (nearest neighbors) values and also range of k-fold value to determine the optimal combination: 

```{r optimalf}
best_fold_finder = function(data, max_k = 20, max_fold = 20){
  
  comparison_list <- list() #empty list to store value
  
  for (i in 1:(max_fold-1)) { 
    results <- accuracy_cal(data, max_k = max_k, k_fold = i + 1) #call the first function, k_fold cannot be 1
    
    comparison_list[[i]]= data.table(fold=i+1,optimal_k = which.max(results), highest_accuracy = max(results))
  }
  
  comparison_table = rbindlist(comparison_list)
  
  return(comparison_table[order(highest_accuracy, decreasing = TRUE)]) #result is a table of folds, corresponding optimal k and accuracy measure
}
```

Let's plug in the credit card data for a range of k (nearest neighbors) of 1 to 20 and maximum folds of 20 for cross validation:

``` {r ccdata}
head(best_fold_finder(cross_train_set,max_k = 20, max_fold = 20)) #display first 6
```

As we can see, the optimal combination is 5 folds cross validation for a knn model of 11 nearest neighbors. The accuracy measure is 85.49%

As the final step, I want to reconfirm the accuracy of the model with k = 11  on the test set:

``` {r xtest_set}
cross_test_knn <- kknn(R1~.,
                  cross_train_set,
                  cross_test_set,
                  k=11, 
                  scale = TRUE)
knn_test_prediction = as.integer(round(fitted(cross_test_knn)))

xtest_knn_accuracy = sum(knn_test_prediction == cross_test_set[,11]) / nrow(cross_test_set)

xtest_knn_accuracy

```

On the test set, a k value of 8 produces an acurracy measure of 85.38%. Hence, this number reflects the 'true' accuracy of the model. 

#### SVM Model

Now that we have done cross validation for KNN model, let's explore the process for SVM model:

From Homework 1, my optimal c value for 'vanilladot' kernel is has a range of values, but let's pick one, 100. Let's cross validate that model.

First, I create a function that will take in the dataframe, c value, and number of fold for cross validation to produce a table that has both unvalidated accuracy measure and validated accuracy measure: 

``` {r crosssvm, results='hide'}
svm_accuracy_cal = function(data, c = 100, k_fold = 10){
  
  set.seed(93)
  cross_model <-  ksvm(as.matrix(data[,1:10]),
                       as.factor(data[,11]),
                       type='C-svc',kernel='vanilladot',
                       C=c,
                       scaled=TRUE,
                       cross = k_fold) #indicate that cross validation to be performed
  
  accuracy_pre_val = 1 - cross_model@error #cross_model@error gives the training error, the unvalidated accuracy measure is 1 minus that
  cross_accuracy = 1 - cross_model@cross #cross_model@cross gives the cross validation error, the validated accuracy measure is 1 minus that
  comparison = data.table(no_validation = accuracy_pre_val, with_validation =  cross_accuracy) 
  
  return(comparison)
}
```

Let's see how the accuracy measure change for c = 100 with 10-fold cross validation on the training set:

``` {r svmexp}
train_set_accuracy = svm_accuracy_cal(cross_train_set, c = 100, k_fold = 10)
```
```{r}
train_set_accuracy
```

On training set, with no validation, accuracy measure is 86.25%. With 10-fold cross validation, the accuracy is 86.08%. The difference is only around 0.15%, which is very small and indicates that the random effects in the training data is small and our model is already optimal.

Similar to knn model, I will write a function that, for a given value of c, loops through a range of k-fold value to determine the optimal combination. Initially, I planned for it to loop through a range of c as well but I've noticed from the last homework that change in c value did little to the accuracy measure unless it was extremely different in magnitude.

``` {r svmformula}

best_fold_finder_svm = function(data, c = 100, max_fold = 20){
  
  svm_comparison_list <- list() #empty list to store value
  
  for(i in 1:(max_fold-1)) {
    set.seed(93)   
    accuracy_measure <-  svm_accuracy_cal(data, c = c, k_fold = i+1) #call the first function, k_fold must be more than 1, otherwise error
     
    svm_comparison_list[[i]] = data.table(fold = i+ 1, no_validation =accuracy_measure[[1]], with_validation = accuracy_measure[[2]], difference = accuracy_measure[[1]] - accuracy_measure[[2]])
  }
  
  svm_comparison_table = rbindlist(svm_comparison_list)
  
  
  return(svm_comparison_table[order(with_validation,decreasing = TRUE)])
}
```

Let's plug in the training set with c = 100 and max_fold = 20:

``` {r svmfold, results='hide'}
fold_table = best_fold_finder_svm(cross_train_set, c = 100, max_fold = 20)
```
``` {r}
head(fold_table) #display first 6
```

As we can see, the optimal number of folds is 20 for cross validation for a 'vanilladot' SVM model with c = 100. The validated accuracy measure is 86.07%.

Finally, let's confirm the accuracy of the svm model with c = 100 on the test set:

``` {r test_set, results='hide'}
xtest_model <- ksvm(as.matrix(cross_train_set[,1:10]),
                   as.factor(cross_train_set[,11]),
                   type='C-svc',
                   kernel='vanilladot',
                   C=100,
                   scaled=TRUE)
```
``` {r}
xpred_test <- predict(xtest_model,cross_test_set[,1:10]) 

xtested_accuracy = sum(xpred_test == cross_test_set[,11]) / nrow(cross_test_set)

xtested_accuracy
```

On the test set, a c value of 100 produces an acurracy measure of 86.92%. Hence, this number reflects the 'true' accuracy of the model.

In order to get the classifier equation for the model, I will have to run the model again on the whole data set.

``` {r xequation, results='hide'}
xoptimal_model = ksvm(as.matrix(mydata[,1:10]),
                     as.factor(mydata[,11]),
                     type='C-svc',
                     kernel='vanilladot',
                     C=100,
                     scaled=TRUE) #set up optimal model

xoptimal_model_a <- colSums(xoptimal_model@xmatrix[[1]] * xoptimal_model@coef[[1]]) #for a1...am

xoptimal_model_a0 <- -xoptimal_model@b #for a0
```

``` {r}
cat('Optimal Linear Model:',xoptimal_model_a[1],"*x1 + ", xoptimal_model_a[2],"*x2 + ",xoptimal_model_a[3],"*x3 + ",xoptimal_model_a[4],"*x4 + ",xoptimal_model_a[5],"*x5 + ",xoptimal_model_a[6],"*x6 + ",xoptimal_model_a[7],"*x7 + ",xoptimal_model_a[8],"*x8 + ",xoptimal_model_a[9],"*x9 + ",xoptimal_model_a[10],"*x10 + ", xoptimal_model_a0,"= 0")
```

### Part b

In this part, I will split the dataset into training, validation, and testing sets for a ratio of 60/20/20 (60% trainig, 20% validation, and 20% testing).

I will employ the splitting method call 'rotation', which will take turns in selecting the data for each set. The order for going through each data point is: training -> validation -> training -> testing -> training for each 5 data points.

I will assign training to a value of 1, validation 2, and testing 3. 

``` {r selection}
distribution <- rep_len(c(1,2,1,3,1),nrow(mydata)) #setting up the selection order by rotation, up to the rows of the dataset
```

Training Set:

``` {r training}
train_set = mydata[distribution==1,] 
```

Validation Set:

```{r validation}
validation_set = mydata[distribution==2,]
```

Testing Set:

``` {r test}
test_set = mydata[distribution==3,]
```

### SVM Model

I will set up 10 values of c with different magnitudes for this question. 

``` {r cvalue}
c_values <- c(0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000)
```

Then I will loop through these c values to conduct training on training set and predict on validation set to determine the optimal c, hence optimal model for testing set:

``` {r svmvalidation, results='hide'}
svm_results <- list()

for (i in 1:length(c_values)){
  set.seed(93)
  train_model <- ksvm(as.matrix(train_set[,1:10]), #train on training set
                      as.factor(train_set[,11]),
                      type='C-svc',kernel='vanilladot',
                      C=c_values[i],
                      scaled=TRUE)
  pred <- predict(train_model,validation_set[,1:10]) #run prediction on validation set
  
  svm_results[[i]] = data.table(c_value = c_values[i], validated_accuracy = (sum(pred == validation_set[,11]) / nrow(validation_set)))
}
```

``` {r result_table}
svm_result_table = rbindlist(svm_results)

head(svm_result_table[order(validated_accuracy, decreasing = TRUE)]) #display first 6
```

Surprisingly, predictions on validation set seem to perform better than on the whole dataset at highest 87.79%.  Maybe there is more random effects in the validation set. Hence, it is neccessary to confirm with the testing set. One of the c value with highest validated accuracy is c = 0.01. Let's use that on testing set.

``` {r testingsvm, results='hide'}
test_model <- ksvm(as.matrix(train_set[,1:10]),
                   as.factor(train_set[,11]),
                   type='C-svc',
                   kernel='vanilladot',
                   C=0.01,
                   scaled=TRUE)

pred_test <- predict(test_model,test_set[,1:10]) 

tested_accuracy = sum(pred_test == test_set[,11]) / nrow(test_set)
```
```{r}
tested_accuracy
```

Accuracy measure seems to normalize on the testing set as 85.50%. As such, we can say that with 60/20/20 split in training, validation, and testing, the model predicts 85.50% correctly with c 

Similarly, in order to get the classifier equation for the model, I will have to run the model again on the whole data set.

``` {r classifer, results='hide'}
optimal_model = ksvm(as.matrix(mydata[,1:10]),
                     as.factor(mydata[,11]),
                     type='C-svc',
                     kernel='vanilladot',
                     C=0.01,
                     scaled=TRUE) #set up optimal model

optimal_model_a <- colSums(optimal_model@xmatrix[[1]] * optimal_model@coef[[1]]) #for a1...am

optimal_model_a0 <- -optimal_model@b #for a0
```

```{r equation}
cat('Optimal Linear Model:',optimal_model_a[1],"*x1 + ", optimal_model_a[2],"*x2 + ",optimal_model_a[3],"*x3 + ",optimal_model_a[4],"*x4 + ",optimal_model_a[5],"*x5 + ",optimal_model_a[6],"*x6 + ",optimal_model_a[7],"*x7 + ",optimal_model_a[8],"*x8 + ",optimal_model_a[9],"*x9 + ",optimal_model_a[10],"*x10 + ", optimal_model_a0,"= 0")
```

### KNN Model

For knn model, I will use training and validation set to loop through a range of k from 1 to 20 to find the optimal k nearest neighbors then use testing set on that.

``` {r trainingsvm}
k_values = c(1:20)

knn_results <- list()

for (i in 1:length(k_values)){
  train_kknn = kknn(R1~.,
               train_set, #train on training set
               validation_set, #predict on validation set
               k=k_values[i], 
               distance = 2, 
               kernel = 'optimal', 
               scale = TRUE)
  pred <- as.integer(round(fitted(train_kknn),0))
  
  knn_results[[i]] = data.table(k_value = k_values[i],validated_accuracy = (sum(pred == validation_set[,11]) / nrow(validation_set)))
}

knn_result_table = rbindlist(knn_results) #turning list of list into table

head(knn_result_table[order(validated_accuracy, decreasing = TRUE)]) #display first 6
```

As we see here, with validation, the optimal value of k is 14, 15, 16, and 17. I will use k = 14 on my testing set to check for the true accuracy of the model:

``` {r testknn}
for (i in 1:nrow(test_set)){
  test_knn_model <- kknn(R1~.,
                    train_set,
                    test_set,
                    k=14, 
                    distance = 2, 
                    kernel = 'optimal', 
                    scale = TRUE)
  test_prediction = as.integer(round(fitted(test_knn_model),0))
}

test_knn_accuracy = sum(test_prediction == test_set[,11]) / nrow(test_set) 

test_knn_accuracy
```

With testing set, the accuracy measure dropped to 82.44% for k = 14, thus reflecting the 'true' accuracy of the model.


## Question 4.1

Every year, I travel to a new city for a week (7 days). During those 7 days, I sometimes find it challenging to utilize my time there to explore as many attractions as possible without advance planning. I usually search online and compile a list of attractions I want to visit, then mark them on a map so I can locate nearby places that I can spend a day around that area.

Now that I learn the concept of clustering, the solution to my travelling problem could be taken to another level by solving it analytically with clustering modeling.

Let's look at an example for my visit to London. My 3 predictors (or 3 axes) will be: 

**1/ x-axis: Distance between the attraction and city centre in meter** (i.e city centre will be Big Ben Tower)

**2/ y-axis: Distance between the attraction and my Airbnb accomodation in meter** (i.e I stayed in Bayswater area)

**3/ z-axis: Distance between the attraction and the Eastern part of the city in metter** (i.e Tower Bridge)

I had about 30 attractions that I wanted to visit in mind. I would collect the data above for each attraction, that would be their coordinate (x,y,z) and mark them on a 3-dimensional graph.

I had 7 days to explore the city, so I would use a 7-Means Clustering model to identify 7 clusters. Then I will spend each day exploring attractions in each clusters to minimize the distance to travel to them from the cluster centroids as well as ensuring I can cover all attractions in my 7 days in London.


## Question 4.2

In this question, I employed 3 clustering models: 1 for all predictors, 1 for sepal predictors and 1 for petal predictors. Then I picked the optimal on by comparing the distance sum of all points to their cluster centers in each model. Optimal model should have the lower distance sum for a given cluster number.

I am also interested in seeing how my models performed compared to actual responses so I pulled them into tables to compare.

```{r setup, results='hide'}
library(ggplot2) #for ploting the cluster
set.seed(93)
```

Calling the iris dataset from R's built-in datasets:

``` {r iris}
data(iris)
head(iris)
```

There are 5 columns with 4 predictors and 1 responses ('Species'). Sepal lengths, sepal widths, petal lengths, and petal widths correspond to column 1, 2, 3, and 4 respectively.

I see that there are differences in magnitude of the values in the 4 predictors, for example, Petal Width values are less than 1. I think it is a good idea to scale the data before the analysis. The scaling here is linear, which gives back data from 0 to 1.

``` {r scale}
scaled_iris <- iris

for (i in 1:4){
  
  scaled_iris[,i] <- (iris[,i] - min(iris[,i])) / (max(iris[,i]) - min(iris[,i]))
}

head(scaled_iris)
```

Of the reponses, I am interested to see how many actual species there are in the dataset:

``` {r species}

summary(iris['Species'])
```

There are total 3 unique species in the dataset with 50 data points in for each.

Now I will build a function that will return a clustering model for a given range of predictor columns and a given value of cluster center:

``` {r model}
cluster_model = function(data, start_col = 1, end_col = 4, center = 2, nstart = 25){
  set.seed(93)
  cluster_model = kmeans(data[,start_col:end_col], center, nstart = nstart)
  return (cluster_model)
}
```

Finally, before I dive into the iris data, I will build a function that will run k-means clustering model on for a given range of predictor columns and a given range of cluster center a return a table that will compare cluster centers and the corresponding sum of distance to the corresponding center.

``` {r compare_cluster}

compare_cluster = function(data, start_col = 1, end_col = 4, min_center = 2, max_center = 5, nstart = 25){
  
  comparison_list <- list()
  
  for (c in min_center:max_center){
    set.seed(93)
    model = cluster_model(data,start_col = start_col, end_col = end_col, center = c, nstart = nstart) #calling the previous function 

    dist_sum = 0 
    
    for (i in 1:nrow(data)){
      
      dist_sum = dist_sum + dist(rbind(data[i,start_col:end_col],model$centers[model$cluster[i],]))[1] #calculate the distance sum between points and their cluster centers
    }
    
    comparison_list[[c]] = data.table(centroid=c, distance_sum=dist_sum)
  }
  
  comparison_table = rbindlist(comparison_list)
  
  return (comparison_table)
}
```

First let's consider the case of using all 4 predictors (column 1 to 4) for clustering:

I will look at a range of possible clusters from 2 to 10 and see which clusters is optimal based on the sum of distance between one datapoints and its cluster:

```{r allpred}

all_pred_table = compare_cluster(scaled_iris,start_col = 1, end_col = 4, min_center = 2, max_center = 10)

all_pred_table
```

To acutally pick the optimal number of centroids, let's plot an Elbow Diagram for the table:

``` {r elbow}

ggplot(all_pred_table,aes(centroid, distance_sum)) + geom_line(color='blue')+ geom_point()
```

The most signficance change in distance sum is from 2 to 3, after that the degree of change is less severe. Hence, the clustering model for all predictors is more optimal with at least 3 cluster centers.

Next, I want to see if using sepal lenghts and widths as predictors only would yield any difference:

``` {r sepal}

sepal_pred_table = compare_cluster(scaled_iris,start_col = 1, end_col = 2, min_center = 2, max_center = 10)

sepal_pred_table

ggplot(sepal_pred_table,aes(centroid, distance_sum)) + geom_line(color='blue')+ geom_point()
```

Similarly to using all predictors, distance sum changes signficantly from from 2 to 3, but after that the degree of change of change is smoother. 

However, I notice that at 3, the distance sum for using Sepal as predictors only is 21.05, while with all predictors it's 29.22. Since the goal is to minimize the distance sum for a given number of cluster, model using sepal predictors only seems to perform better than one with all predictors.

Lastly, I will run the same thing again, this time using petal lengths and petal widths. Technically since there are 4 predictors, if I want a clustering model that runs on 2 predictors I could have 6 possible combinations. I just do not think that it makes since to run a model on, for example, sepal lengths and petal widths.


``` {r petal}

petal_pred_table = compare_cluster(scaled_iris,start_col = 3, end_col = 4, min_center = 2, max_center = 10)

petal_pred_table

ggplot(petal_pred_table,aes(centroid, distance_sum)) + geom_line(color='blue')+ geom_point()
```

Same as the above, this model would be optimal with at least 3 cluster centers. 

But the important thing is that at 3 clusters, this model produces the least distance sum at 13.34 among the 3 models that I ran. Therefore, I conclude that the clustering model using only petal widths and petal lengths as predictors is more optimal.

Now that I have determined the optimal model, let's run and plot the scatter plot of this model for 3 cluster centers to compare with the actual responses. This is for the sake of the analysis that I picked 3 cluster centers to be the same with the number of actual species in this dataset. In real life, actual responses should not be known since clustering is unsupervised learning.

``` {r plot}

petal_model = cluster_model(scaled_iris, start_col = 3, end_col = 4, center = 3)
petal_model
```

Let's now, for the sake of the exercise, plot a table to compared between predicted cluster of our 3 models above and actual responses:

``` {r comps}
all_model = cluster_model(scaled_iris, start_col = 1, end_col = 4, center = 3)
sepal_model = cluster_model(scaled_iris, start_col = 1, end_col = 2, center = 3)

table(all_model$cluster, scaled_iris$Species) #combining predicted clusters and actual species names together

table(sepal_model$cluster, scaled_iris$Species)

table(petal_model$cluster, scaled_iris$Species)
```

All-predictor model misclassified 14 virginca, 3 versicolor while sepal-predictor model misclassified 16 virginaca and 13 versicolor.

The 3 clusters size of petal model are 48, 52, 50. Compared to the actual species size in the data of 50, 50, 50. It seems that only 2 'versicolor' datapoints and 4 'virginca' are misclassified with 'setosa' perfecly classified. Hence, this reassures that the clustering model with petal lengths and petal widths as the predictors is optimal.

``` {r scatter}
petal_model$cluster <- as.factor(petal_model$cluster) #change cluster resutls from continuous to discrete scale

ggplot(scaled_iris, aes(Petal.Length, Petal.Width, color = petal_model$cluster)) + geom_point()
```


The scatter plot seems very clean and each data points colored by their predicted cluster.



