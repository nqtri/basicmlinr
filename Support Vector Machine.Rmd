## Credit Approval Data Set
The dataset is the “Credit Approval Data Set” from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Credit+Approval) without the categorical variables and without data points that have missing values

Setting up by calling relevant libraries and opening data file and assigning to mydata variable:

```{r set up, results='hide}
library(kernlab)
library(data.table)
mydata <- read.table('credit_card_data-headers.txt',header = TRUE)
```

Here, I want to test for different magnitude ranges of C to check which one would give the higest accuracy in the Support Vector Machine model. Hence, I create a vector containing all C values that will be tested, ranging from 0.000001 to 1,000,000:

``` {r c_values}

c_values <- c(seq(0.000001, 0.00001, by = 0.000001),
            seq(0.00002, 0.0001, by = 0.00001),
            seq(0.0002, 0.001, by = 0.0001),
            seq(0.002, 0.01, by = 0.001),
            seq(0.02, 0.1, by = 0.01),
            seq(0.2, 1, by = 0.1),
            seq(2, 10, by = 1),
            seq(20, 100, by = 10),
            seq(200, 1000, by = 100),
            seq(2000, 10000, by = 1000),
            seq(20000, 100000, by = 10000),
            seq(200000, 1000000, by = 100000),
            seq(2000000,10000000, by = 1000000))

length(c_values)
```

There are total 118 C values to be tested. 

Creating an empty list to store prediction accuracy measurements:

``` {r accuracy_list}
accuracy_list <- list()
```

Calling Support Vector Machine function: ksvm and loop it with different C values in c_values:

```{r svm, results='hide'}
for(i in 1:length(c_values)){
  model <- ksvm(as.matrix(mydata[,1:10]),as.factor(mydata[,11]),type='C-svc',kernel='vanilladot',C=c_values[[i]],scaled=TRUE) #setting up the model
  
  pred <- predict(model,mydata[,1:10]) #running prediction
  
  accuracy_list[[i]]= data.table(c=c_values[[i]],accuracy=sum(pred == mydata[,11]) / nrow(mydata)) #measuring accuracy and assigning the results to accuracy_list
}
```
Sorting accuracy measurements by descending value and displaying the top 10 C values that gives the best prediction: 

```{r accuracy_rank}
results_dt_test = rbindlist(accuracy_list)

head(results_dt_test[order(-accuracy)][1:50]) #head of the best 50 C values in the model
tail(results_dt_test[order(-accuracy)][1:50]) #tail of the best 50 C values in the model
```

It seems like for a range of c values range from 0.002 to 100,000 gives the best prediction for the credit approval data, with more than 86% accuracy. However, it is a caution to note that not every point of data in this range will give the exact same results.

I am also interested to see which C values would give the least accurate prediction:
```{r worst_c}
results_dt_test[order(accuracy)][1:5]
```

Now that I have optimal C values in our range, I will go find the coefficients a1...am as well as a0, i will pick the top C value displayed: 0.002

```{r optimal_model}
optimal_model = ksvm(as.matrix(mydata[,1:10]),as.factor(mydata[,11]),type='C-svc',kernel='vanilladot',C=0.002,scaled=TRUE)

optimal_model_a <- colSums(optimal_model@xmatrix[[1]] * optimal_model@coef[[1]])

optimal_model_a0 <- -optimal_model@b
```

a1...am are:

``` {r a1am}
optimal_model_a
```

a0 is:

``` {r a0}
optimal_model_a0
```

Summarizing the linear equation for the optimal model: 

``` {r equation, echo = FALSE}
cat('Optimal Model:',optimal_model_a[1],"*x1 + ", optimal_model_a[2],"*x2 + ",optimal_model_a[3],"*x3 + ",optimal_model_a[4],"*x4 + ",optimal_model_a[5],"*x5 + ",optimal_model_a[6],"*x6 + ",optimal_model_a[7],"*x7 + ",optimal_model_a[8],"*x8 + ",optimal_model_a[9],"*x9 + ",optimal_model_a[10],"*x10 + ", optimal_model_a0,"= 0")
```

I was curious to see what will happen for models with non-linear kernels. After some research online, I read the 'rbfdot' (Radial Basis) kernel is a good default kernel for non-linear model so I decided to give it a try.

Calling Support Vector Machine function: ksvm with 'rbfdot' kernel (non-linear) and C = 500:

``` {r rbfdotmodel}
rbfdot_model <- ksvm (as.matrix(mydata[,1:10]),as.factor(mydata[,11]),type='C-svc',kernel='rbfdot',C=500,scaled=TRUE)

rbfdot_model
```

Calculating a1...am in Radial Basis Model:

``` {r rbfdota}
rbfdot_model_a <- colSums(rbfdot_model@xmatrix[[1]] * rbfdot_model@coef[[1]])

rbfdot_model_a
```

Calculating a0 in Radial Basis Model:

``` {r rbfdota0}
rbfdot_model_a0 <- -rbfdot_model@b
```

Looking at the predictions by Radial Basis Model:

``` {r rbfdotpredict}
rbfdot_model_pred <- predict(rbfdot_model,mydata[,1:10])
```

Calculating the fraction of Radial Basis Model’s predictions match the actual classification, hence reflecting the accuracy level:

``` {r rbfdotaccurary}
rbfdot_model_accuracy <- sum(rbfdot_model_pred == mydata[,11]) / nrow(mydata)

rbfdot_model_accuracy
```

``` {r rbfdotstatement, echo = FALSE}
print(paste("Accuracy of the model with Radial Basis kernel and C = 500: ", rbfdot_model_accuracy))
```

The model of 'rbdot' kernel surprised me with almost 98% prediction accuracy, much better than the best of the above 'vanilladot' kernel model of about 86%. This might explain that the prediction model for credit card approval we are looking at should be non-linear.




